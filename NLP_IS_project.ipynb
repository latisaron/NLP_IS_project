{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fdece38",
   "metadata": {},
   "source": [
    "Import libraries for preprocessing & download necessary nltk \"packages\" ( I don't know how else to call them )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84daee09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22affb62",
   "metadata": {},
   "source": [
    "Define function to be applied to the dataframe for preprocessing.\n",
    "We go through the usual flow of analyzing the text, tokenizing it and normalizing it in this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5448b945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(full_string, stop_words, normalizer):\n",
    "    # I removed all non-ascii characters because the dataset had a lot of character which were probably emojis\n",
    "    # but were not translated properly. They would not have brought any sort of benefits.\n",
    "    ascii_full_string = full_string.encode(\"ascii\", \"ignore\").decode()\n",
    "    # Tokenization with punkt\n",
    "    tokenized_string = wordpunct_tokenize(ascii_full_string)\n",
    "    # Removal of stop words which would not bring any benefit\n",
    "    tokens_without_stopwords = [word for word in tokenized_string if word not in stop_words]\n",
    "    # Depending on the normalization type the preprocessing part either uses a stemmer or a lemmatizer\n",
    "    if type(normalizer) == nltk.stem.porter.PorterStemmer:\n",
    "        singles = [normalizer.stem(token) for token in tokens_without_stopwords]\n",
    "    if type(normalizer) == nltk.stem.wordnet.WordNetLemmatizer:\n",
    "        singles = [normalizer.lemmatize(token) for token in tokens_without_stopwords]\n",
    "    # Join the string for further processing\n",
    "    final_string = (' ').join(singles)\n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82e5944",
   "metadata": {},
   "source": [
    "Read & preprocess the data.\n",
    "The first step is to read the csv from a local file. \n",
    "After that, during the data exploration part of the project, I noticed that there is a lack of balance in terms of label representation. ( ~29300 labels characterized as not racist - 2100 labels characetized as racist )\n",
    "Despite this I still trained the model with this initial dataframe, but the results were very skewed towards the label which was better represented (obviously).\n",
    "This is why I chose to pick the same number of records from label 0 as the ones from label 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b209e0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\".//train.csv\", header = 'infer', sep = \",\", index_col = \"id\")\n",
    "class_1_df = df[df[\"label\"] == 1]\n",
    "class_0_df = df[df[\"label\"] == 0].sample(frac = 1)[ : len(class_1_df)]\n",
    "df = pd.concat([class_0_df, class_1_df]).sample(frac = 1)\n",
    "\n",
    "class_col = df.pop(\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cde4c2",
   "metadata": {},
   "source": [
    "Define global variables for future uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a51ba000",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "normalizer_hash = {\n",
    "    \"stemmer\": PorterStemmer(),\n",
    "    \"lemmatizer\": WordNetLemmatizer()\n",
    "    }\n",
    "vectorizer_hash = {\n",
    "    \"tfidf\": TfidfVectorizer(),\n",
    "    \"count\": CountVectorizer()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33e06a4",
   "metadata": {},
   "source": [
    "Use the tokenize() function and then go forward in the flow by vectorizing the manipulated strings and selecting the best features for the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e16eb671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(df_input, normalizer, vectorizer):\n",
    "    df = df_input.copy()\n",
    "    # Initial tokenization step\n",
    "    df[\"tweet\"] = [tokenize(row, stop_words, normalizer_hash[normalizer]) for row in df[\"tweet\"]]\n",
    "\n",
    "    # Initial vectorizer result\n",
    "    x = vectorizer_hash[vectorizer].fit_transform(df[\"tweet\"])\n",
    "    # Choosing the best 50 values\n",
    "    ch2 = SelectKBest(chi2, k = 50)\n",
    "    ch2.fit(x, class_col)\n",
    "    # Mask containing the features I should be using in the model\n",
    "    mask = ch2.get_support()\n",
    "\n",
    "    # Going from text to a list of numbers\n",
    "    df[\"tweet\"] = list(x.toarray())\n",
    "    # Choosing what specific numbers to retain from that list\n",
    "    df[\"tweet\"] = [ row[mask] for row in df[\"tweet\"] ]\n",
    "    # Create new feature names\n",
    "    column_names = [\"feature\" + str(i) for i in range(len(df[\"tweet\"].iloc[1]))]\n",
    "    # Go from df with a column containing a list to a df with multiple features.\n",
    "    df_preprocessed = pd.DataFrame(df.tweet.to_list(), columns = column_names, index = df.index)\n",
    "\n",
    "    # Split the data for testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_preprocessed, class_col, test_size=0.2, random_state=9, stratify=class_col)\n",
    "    return [X_train, X_test, y_train, y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838905ec",
   "metadata": {},
   "source": [
    "Choose one of the four preprocessing setups from below and run the respective cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076d072c",
   "metadata": {},
   "source": [
    "Preprocess data with Stemming and CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3faa6eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_dataframe(df, \"stemmer\", \"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c40db73",
   "metadata": {},
   "source": [
    "Preprocess data with Stemming and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34925c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_dataframe(df, \"stemmer\", \"tfidf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616b6232",
   "metadata": {},
   "source": [
    "Preprocess data with Lemmatizer and CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7c765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_dataframe(df, \"lemmatizer\", \"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d112023e",
   "metadata": {},
   "source": [
    "Preprocess data with Lemmatizer and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b32768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_dataframe(df, \"lemmatizer\", \"tfidf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7cb18f",
   "metadata": {},
   "source": [
    "Define model evaluation metrics & import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c779f564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def metrics(y_test, y_pred, alg):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp)    \n",
    "    sensitivity = tp / (tp + fn)\n",
    "    accuracy = (tp+tn) / (tp + fp + tn + fn)\n",
    "    print(\"============== {} ==============\".format(alg))\n",
    "    print(\"Accuracy has a value of {}\".format(accuracy))\n",
    "    print(\"Specificity has a value of {}\".format(specificity))\n",
    "    print(\"Sensitivity has a value of {}\".format(sensitivity))\n",
    "    print( pd.DataFrame(\n",
    "        confusion_matrix(y_test, y_pred), \n",
    "        columns = ['Prediction 0', 'Prediction 1'],\n",
    "        index = ['True 0', 'True 1']))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bcec69",
   "metadata": {},
   "source": [
    "Define a simple SVM model.\n",
    "Given that SVC automatically gets an rbf kernel, other input parameters except C are not applicable.\n",
    "Therefore, I chose to go with a crossvalidation-trained model with a grid search over parameter C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b072039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== svm ==============\n",
      "Accuracy has a value of 0.7447045707915273\n",
      "Specificity has a value of 0.8530066815144766\n",
      "Sensitivity has a value of 0.6361607142857143\n",
      "        Prediction 0  Prediction 1\n",
      "True 0           383            66\n",
      "True 1           163           285\n",
      " \n"
     ]
    }
   ],
   "source": [
    "svm = SVC(gamma = 'auto', probability = True)\n",
    "parameters = {\n",
    "            'C': [5, 10, 15]\n",
    "}\n",
    "clf = GridSearchCV(svm, parameters, cv=5)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_svm = clf.predict(X_test)\n",
    "\n",
    "metrics(y_test, y_pred_svm, \"svm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6449d0ec",
   "metadata": {},
   "source": [
    "Define a simple Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ca2f07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== naive bayes ==============\n",
      "Accuracy has a value of 0.6867335562987736\n",
      "Specificity has a value of 0.42538975501113585\n",
      "Sensitivity has a value of 0.9486607142857143\n",
      "        Prediction 0  Prediction 1\n",
      "True 0           191           258\n",
      "True 1            23           425\n",
      " \n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred_nb = gnb.predict(X_test)\n",
    "\n",
    "metrics(y_test, y_pred_nb, \"naive bayes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f620f9b8",
   "metadata": {},
   "source": [
    "Import necessary libraries for a simple MLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "38a9611b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import ReLU\n",
    "from torch.nn import Module\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Linear\n",
    "from torch import Tensor\n",
    "from torch.utils.data import *\n",
    "from torch import LongTensor\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn.init import xavier_uniform_\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea9a71c",
   "metadata": {},
   "source": [
    "Define the neural network class.\n",
    "Given that MLPs can also be used for classification I chose to also try this model just as an experiment.\n",
    "It's created of 3 hidden layers, all of their weights being initialized with a Xavier Uniform initialization.\n",
    "The 3 hidden layers have 25, 25 and 10 neurons each, with a ReLU activation function.\n",
    "The input layer has 50 neurons (given that I chose to use 50 features).\n",
    "The output layer has a single neuron computed using a sigmoid activation function. Then the result is rounded to either 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eae1dc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.learning_rate = 1e-3\n",
    "        self.hidden_layer1 = Linear(50, 25)\n",
    "        xavier_uniform_(self.hidden_layer1.weight)\n",
    "        self.act1 = ReLU()\n",
    "        self.hidden_layer2 = Linear(25, 25)\n",
    "        xavier_uniform_(self.hidden_layer2.weight)\n",
    "        self.act2 = ReLU()\n",
    "        self.hidden_layer3 = Linear(25, 10)\n",
    "        xavier_uniform_(self.hidden_layer3.weight)\n",
    "        self.act3 = ReLU()\n",
    "        self.hidden_layer4 = Linear(10, 1)\n",
    "        xavier_uniform_(self.hidden_layer3.weight)\n",
    "        self.act4 = Sigmoid()\n",
    "        \n",
    "    def forward(self, data):\n",
    "        data = self.hidden_layer1(data)\n",
    "        data = self.act1(data)\n",
    "        data = self.hidden_layer2(data)\n",
    "        data = self.act2(data)\n",
    "        data = self.hidden_layer3(data)\n",
    "        data = self.act3(data)\n",
    "        data = self.hidden_layer4(data)\n",
    "        data = self.act4(data)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a49c21",
   "metadata": {},
   "source": [
    "Reshape input data, create model, train model, predict the results and evaluate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eff7ed32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== mlp ==============\n",
      "Accuracy has a value of 0.633221850613155\n",
      "Specificity has a value of 0.5924276169265034\n",
      "Sensitivity has a value of 0.6741071428571429\n",
      "        Prediction 0  Prediction 1\n",
      "True 0           266           183\n",
      "True 1           146           302\n",
      " \n"
     ]
    }
   ],
   "source": [
    "train = TensorDataset(Tensor(np.array(X_train)), LongTensor(np.array(y_train)))\n",
    "train_dl = DataLoader(train, batch_size = 16)\n",
    "\n",
    "mlp = MLP()    \n",
    "optimizer = SGD(mlp.parameters(), lr = 1e-3)\n",
    "criterion = BCELoss()\n",
    "\n",
    "# training\n",
    "for epoch in range(100):\n",
    "    for i, data in enumerate(train_dl, 0):\n",
    "        inputs, targets = data\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = mlp(inputs)\n",
    "        targets = targets.unsqueeze(1)\n",
    "        targets = targets.float()\n",
    "        loss = criterion(y_pred, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# ugly testing\n",
    "y_pred_mlp = []\n",
    "for _, item in X_test.iterrows():\n",
    "    if type(item) == str:\n",
    "        continue\n",
    "    y_pred_mlp.append(np.round(mlp(Tensor(np.array(item)))[0].item()))\n",
    "\n",
    "metrics(y_test, y_pred_mlp, \"mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927cf24d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
