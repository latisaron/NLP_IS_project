{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a26a62e7",
   "metadata": {},
   "source": [
    "Import libraries for preprocessing & download necessary nltk \"packages\" ( I don't know how else to call them )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab257dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c3d499",
   "metadata": {},
   "source": [
    "Define function to be applied to the dataframe for preprocessing.\n",
    "We go through the usual flow of analyzing the text, tokenizing it and normalizing it in this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadf8e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(full_string, stop_words, normalizer):\n",
    "    # I removed all non-ascii characters because the dataset had a lot of character which were probably emojis\n",
    "    # but were not translated properly. They would not have brought any sort of benefits.\n",
    "    ascii_full_string = full_string.encode(\"ascii\", \"ignore\").decode()\n",
    "    # Tokenization with punkt\n",
    "    tokenized_string = wordpunct_tokenize(ascii_full_string)\n",
    "    # Removal of stop words which would not bring any benefit\n",
    "    tokens_without_stopwords = [word for word in tokenized_string if word not in stop_words]\n",
    "    # Depending on the normalization type the preprocessing part either uses a stemmer or a lemmatizer\n",
    "    if type(normalizer) == nltk.stem.porter.PorterStemmer:\n",
    "        singles = [normalizer.stem(token) for token in tokens_without_stopwords]\n",
    "    if type(normalizer) == nltk.stem.wordnet.WordNetLemmatizer:\n",
    "        singles = [normalizer.lemmatize(token) for token in tokens_without_stopwords]\n",
    "    # Join the string for further processing\n",
    "    final_string = (' ').join(singles)\n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bc78ff",
   "metadata": {},
   "source": [
    "Read & preprocess the data.\n",
    "The first step is to read the csv from a local file. \n",
    "After that, during the data exploration part of the project, I noticed that there is a lack of balance in terms of label representation. ( ~29300 labels characterized as not racist - 2100 labels characetized as racist )\n",
    "Despite this I still trained the model with this initial dataframe, but the results were very skewed towards the label which was better represented (obviously).\n",
    "This is why I chose to pick the same number of records from label 0 as the ones from label 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7318ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\".//train.csv\", header = 'infer', sep = \",\", index_col = \"id\")\n",
    "class_1_df = df[df[\"label\"] == 1]\n",
    "class_0_df = df[df[\"label\"] == 0].sample(frac = 1)[ : len(class_1_df)]\n",
    "df = pd.concat([class_0_df, class_1_df]).sample(frac = 1)\n",
    "\n",
    "class_col = df.pop(\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d855f2a",
   "metadata": {},
   "source": [
    "Define global variables for future uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d8f655",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "normalizer_hash = {\n",
    "    \"stemmer\": PorterStemmer(),\n",
    "    \"lemmatizer\": WordNetLemmatizer()\n",
    "    }\n",
    "vectorizer_hash = {\n",
    "    \"tfidf\": TfidfVectorizer(),\n",
    "    \"count\": CountVectorizer()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e712b9b1",
   "metadata": {},
   "source": [
    "Use the tokenize() function and then go forward in the flow by vectorizing the manipulated strings and selecting the best features for the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d65c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial tokenization step\n",
    "df[\"tweet\"] = [tokenize(row, stop_words, normalizer_hash[\"lemmatizer\"]) for row in df[\"tweet\"]]\n",
    "\n",
    "# Initial vectorizer result\n",
    "x = vectorizer_hash[\"count\"].fit_transform(df[\"tweet\"])\n",
    "# Choosing the best 50 values\n",
    "ch2 = SelectKBest(chi2, k = 50)\n",
    "ch2.fit(x, class_col)\n",
    "# Mask containing the features I should be using in the model\n",
    "mask = ch2.get_support()\n",
    "\n",
    "# Going from text to a list of numbers\n",
    "df[\"tweet\"] = list(x.toarray())\n",
    "# Choosing what specific numbers to retain from that list\n",
    "df[\"tweet\"] = [ row[mask] for row in df[\"tweet\"] ]\n",
    "# Create new feature names\n",
    "column_names = [\"feature\" + str(i) for i in range(len(df[\"tweet\"].iloc[1]))]\n",
    "# Go from df with a column containing a list to a df with multiple features.\n",
    "df_preprocessed = pd.DataFrame(df.tweet.to_list(), columns = column_names, index = df.index)\n",
    "\n",
    "# Split the data for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_preprocessed, class_col, test_size=0.2, random_state=9, stratify=class_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805b3571",
   "metadata": {},
   "source": [
    "Define model evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0595e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_test, y_pred, alg):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp)    \n",
    "    sensitivity = tp / (tp + fn)\n",
    "    accuracy = (tp+tn) / (tp + fp + tn + fn)\n",
    "    print(\"============== {} ==============\".format(alg))\n",
    "    print(\"Accuracy has a value of {}\".format(accuracy))\n",
    "    print(\"Specificity has a value of {}\".format(specificity))\n",
    "    print(\"Sensitivity has a value of {}\".format(sensitivity))\n",
    "    print( pd.DataFrame(\n",
    "        confusion_matrix(y_test, y_pred), \n",
    "        columns = ['Prediction 0', 'Prediction 1'],\n",
    "        index = ['True 0', 'True 1']))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fd9985",
   "metadata": {},
   "source": [
    "Define a simple SVM model.\n",
    "Given that SVC automatically gets an rbf kernel, other input parameters except C are not applicable.\n",
    "Therefore, I chose to go with a crossvalidation-trained model with a grid search over parameter C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e82115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(gamma = 'auto', probability = True)\n",
    "parameters = {\n",
    "            'C': [5, 10, 15]\n",
    "}\n",
    "clf = GridSearchCV(svm, parameters, cv=5)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_svm = clf.predict(X_test)\n",
    "\n",
    "metrics(y_test, y_pred_svm, \"svm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95465db",
   "metadata": {},
   "source": [
    "Define a simple Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee7ad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred_nb = gnb.predict(X_test)\n",
    "\n",
    "metrics(y_test, y_pred_nb, \"naive bayes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4596bba",
   "metadata": {},
   "source": [
    "Import necessary libraries for a simple MLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d4c5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import ReLU\n",
    "from torch.nn import Module\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Linear\n",
    "from torch import Tensor\n",
    "from torch.utils.data import *\n",
    "from torch import LongTensor\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn.init import xavier_uniform_\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab074a5f",
   "metadata": {},
   "source": [
    "Define the neural network class.\n",
    "Given that MLPs can also be used for classification I chose to also try this model just as an experiment.\n",
    "It's created of 3 hidden layers, all of their weights being initialized with a Xavier Uniform initialization.\n",
    "The 3 hidden layers have 25, 25 and 10 neurons each, with a ReLU activation function.\n",
    "The input layer has 50 neurons (given that I chose to use 50 features).\n",
    "The output layer has a single neuron computed using a sigmoid activation function. Then the result is rounded to either 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00637ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.learning_rate = 1e-3\n",
    "        self.hidden_layer1 = Linear(50, 25)\n",
    "        xavier_uniform_(self.hidden_layer1.weight)\n",
    "        self.act1 = ReLU()\n",
    "        self.hidden_layer2 = Linear(25, 25)\n",
    "        xavier_uniform_(self.hidden_layer2.weight)\n",
    "        self.act2 = ReLU()\n",
    "        self.hidden_layer3 = Linear(25, 10)\n",
    "        xavier_uniform_(self.hidden_layer3.weight)\n",
    "        self.act3 = ReLU()\n",
    "        self.hidden_layer4 = Linear(10, 1)\n",
    "        xavier_uniform_(self.hidden_layer3.weight)\n",
    "        self.act4 = Sigmoid()\n",
    "        \n",
    "    def forward(self, data):\n",
    "        data = self.hidden_layer1(data)\n",
    "        data = self.act1(data)\n",
    "        data = self.hidden_layer2(data)\n",
    "        data = self.act2(data)\n",
    "        data = self.hidden_layer3(data)\n",
    "        data = self.act3(data)\n",
    "        data = self.hidden_layer4(data)\n",
    "        data = self.act4(data)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea5c37d",
   "metadata": {},
   "source": [
    "Reshape input data, create model, train model, predict the results and evaluate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b15ec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP()    \n",
    "optimizer = SGD(mlp.parameters(), lr = 1e-3)\n",
    "criterion = BCELoss()\n",
    "\n",
    "# training\n",
    "for epoch in range(100):\n",
    "    for i, data in enumerate(train_dl, 0):\n",
    "        inputs, targets = data\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = mlp(inputs)\n",
    "        targets = targets.unsqueeze(1)\n",
    "        targets = targets.float()\n",
    "        loss = criterion(y_pred, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# ugly testing\n",
    "y_pred_mlp = []\n",
    "for _, item in X_test.iterrows():\n",
    "    if type(item) == str:\n",
    "        continue\n",
    "    y_pred_mlp.append(np.round(mlp(Tensor(np.array(item)))[0].item()))\n",
    "\n",
    "metrics(y_test, y_pred_mlp, \"mlp\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
